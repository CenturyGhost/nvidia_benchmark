{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f7ef15",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-15T20:49:44.879656Z",
     "iopub.status.busy": "2024-11-15T20:49:44.878953Z",
     "iopub.status.idle": "2024-11-15T20:49:45.984976Z",
     "shell.execute_reply": "2024-11-15T20:49:45.983812Z"
    },
    "papermill": {
     "duration": 1.113012,
     "end_time": "2024-11-15T20:49:45.987628",
     "exception": false,
     "start_time": "2024-11-15T20:49:44.874616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683ed8d",
   "metadata": {
    "papermill": {
     "duration": 0.001683,
     "end_time": "2024-11-15T20:49:45.991687",
     "exception": false,
     "start_time": "2024-11-15T20:49:45.990004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# *NVIDIA GPU GUIDE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f38eaa",
   "metadata": {
    "papermill": {
     "duration": 0.00158,
     "end_time": "2024-11-15T20:49:45.995110",
     "exception": false,
     "start_time": "2024-11-15T20:49:45.993530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session for the master node\n",
    "# 1 - Use all available local cores for the master node\n",
    "# 2 - Set the application name for the master node\n",
    "# 3 - Configure the driver and executor memory and GPU settings\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Configure Spark with GPU settings for RTX 4090 (24GB) and A5000 (24GB)\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ERROR 418 - I'M A TEAPOT\") \\\n",
    "    .config(\"spark.driver.memory\", \"50g\") \\\n",
    "    .config(\"spark.executor.memory\", \"50g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"50g\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.pinnedPool.size\", \"24G\") \\\n",
    "    .config(\"spark.rapids.sql.concurrentGpuTasks\", \"2\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.pooling.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.allocFraction\", \"0.95\") \\\n",
    "    .config(\"spark.rapids.sql.explain\", \"ALL\") \\\n",
    "    .config(\"spark.executor.resource.gpu.amount\", \"2\") \\\n",
    "    .config(\"spark.task.resource.gpu.amount\", \"0.25\") \\\n",
    "    .config(\"spark.rapids.sql.incompatibleOps.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.host.spillStorageSize\", \"64G\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.maxAllocFraction\", \"0.95\") \\\n",
    "    .config(\"spark.rapids.sql.batchSizeBytes\", \"512M\") \\\n",
    "    .config(\"spark.rapids.sql.reader.batchSizeRows\", \"100000\") \\\n",
    "    .config(\"spark.rapids.sql.variableRowGroupSize.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "if gpus:\n",
    "  # Replicate your computation on multiple GPUs\n",
    "  c = []\n",
    "  for gpu in gpus:\n",
    "    with tf.device(gpu.name):\n",
    "      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "      c.append(tf.matmul(a, b))\n",
    "\n",
    "  with tf.device('/CPU:0'):\n",
    "    matmul_sum = tf.add_n(c)\n",
    "\n",
    "  print(matmul_sum)\n",
    "\n",
    "# Set log level to INFO\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "# Configure logging\n",
    "log4jLogger = spark._jvm.org.apache.log4j\n",
    "logger = log4jLogger.LogManager.getLogger(__name__)\n",
    "\n",
    "def custom_logger(level, message):\n",
    "    color = 'white'\n",
    "    if level == \"INFO\":\n",
    "        color = 'cyan'\n",
    "    elif level == \"SUCCESS\":\n",
    "        color = 'green'\n",
    "    elif level == \"ERROR\":\n",
    "        color = 'red'\n",
    "    elif level == \"ACTION\":\n",
    "        color = 'blue'\n",
    "    elif level == \"PROGRESS\" or level == \"WARNING\":\n",
    "        color = 'yellow'\n",
    "    elif level == \"FINAL\":\n",
    "        color = 'magenta'\n",
    "    logger.info(colored(f\"SPARK: {level} - {message}\", color))\n",
    "\n",
    "# Set the environment variable for memory allocator\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "\n",
    "def initialize_cuda():\n",
    "    cuda = ctypes.CDLL('libcuda.so')\n",
    "    cuInit = cuda.cuInit\n",
    "    cuInit.restype = ctypes.c_int\n",
    "    cuInit.argtypes = [ctypes.c_uint]\n",
    "    cuDeviceGetCount = cuda.cuDeviceGetCount\n",
    "    cuDeviceGetCount.restype = ctypes.c_int\n",
    "    cuDeviceGetCount.argtypes = [ctypes.POINTER(ctypes.c_int)]\n",
    "    cuDeviceGet = cuda.cuDeviceGet\n",
    "    cuDeviceGet.restype = ctypes.c_int\n",
    "    cuDeviceGet.argtypes = [ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n",
    "\n",
    "    res = cuInit(0)\n",
    "    if res != 0:\n",
    "        raise RuntimeError(\"Failed to initialize CUDA\")\n",
    "    device_count = ctypes.c_int()\n",
    "    res = cuDeviceGetCount(ctypes.byref(device_count))\n",
    "    if res != 0:\n",
    "        raise RuntimeError(\"Failed to get device count\")\n",
    "    devices = []\n",
    "    for i in range(device_count.value):\n",
    "        device = ctypes.c_int()\n",
    "        res = cuDeviceGet(ctypes.byref(device), i)\n",
    "        if res != 0:\n",
    "            raise RuntimeError(f\"Failed to get device {i}\")\n",
    "        devices.append(device.value)\n",
    "    return devices\n",
    "\n",
    "def allocate_gpu_memory(size):\n",
    "    cuda = ctypes.CDLL('libcuda.so')\n",
    "    cuda_malloc = cuda.cuMemAlloc\n",
    "    cuda_malloc.restype = ctypes.c_int\n",
    "    cuda_malloc.argtypes = [ctypes.POINTER(ctypes.c_ulonglong), ctypes.c_ulonglong]\n",
    "    ptr = ctypes.c_ulonglong()\n",
    "    res = cuda_malloc(ctypes.byref(ptr), size)\n",
    "    if res != 0:\n",
    "        raise RuntimeError(\"Failed to allocate GPU memory\")\n",
    "    return ptr.value\n",
    "\n",
    "def free_gpu_memory(ptr):\n",
    "    cuda = ctypes.CDLL('libcuda.so')\n",
    "    cuda_free = cuda.cuMemFree\n",
    "    cuda_free.restype = ctypes.c_int\n",
    "    cuda_free.argtypes = [ctypes.c_ulonglong]\n",
    "    res = cuda_free(ptr)\n",
    "    if res != 0:\n",
    "        raise RuntimeError(\"Failed to free GPU memory\")\n",
    "\n",
    "# Check GPU memory usage\n",
    "def check_gpu_memory():\n",
    "    os.system('nvidia-smi')\n",
    "\n",
    "# Initialize CUDA\n",
    "cuda_devices = initialize_cuda()\n",
    "\n",
    "# Check available GPU memory\n",
    "check_gpu_memory()\n",
    "\n",
    "# Ensure TensorFlow uses the GPU by setting memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting memory growth: {e}\")\n",
    "\n",
    "# Check and print the number of GPUs available\n",
    "num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f\"Num GPUs Available: {num_gpus}\")\n",
    "\n",
    "# Perform a simple TensorFlow operation to verify GPU usage\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=tf.float32)\n",
    "    c = tf.matmul(a, b)\n",
    "print(f\"Result of matrix multiplication on GPU:\\n{c.numpy()}\")\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        memory_used = int(result.stdout.decode('utf-8').strip())\n",
    "        return memory_used\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get GPU memory usage: {e}\")\n",
    "        return None\n",
    "\n",
    "def freegpu():\n",
    "    try:\n",
    "        # Get GPU memory usage before freeing\n",
    "        memory_before = get_gpu_memory_usage()\n",
    "\n",
    "        result = subprocess.run(['sudo', 'fuser', '-v', '/dev/nvidia*'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        for line in result.stdout.decode('utf-8').split('\\n'):\n",
    "            if '/dev/nvidia' in line:\n",
    "                pid = int(line.split()[-1])\n",
    "                os.kill(pid, 9)\n",
    "\n",
    "        # Get GPU memory usage after freeing\n",
    "        memory_after = get_gpu_memory_usage()\n",
    "\n",
    "        if memory_before is not None and memory_after is not None:\n",
    "            memory_freed = memory_before - memory_after\n",
    "            print(f\"GPU memory has been freed. {memory_freed} MB of GPU memory was released.\")\n",
    "        else:\n",
    "            print(\"GPU memory has been freed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to free GPU memory: {e}\")\n",
    "\n",
    "# Set the environment variable for XLA flags\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'\n",
    "\n",
    "# ======================================================================================================\n",
    "# ==============================[SETUP MIRROREDSTRATEGY WITHOUT NCCL]===================================\n",
    "# ======================================================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Define the strategy with HierarchicalCopyAllReduce to avoid NCCL\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "strategy = tf.distribute.MirroredStrategy(\n",
    "    devices=[\"/gpu:0\", \"/gpu:1\"],\n",
    "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()\n",
    ")\n",
    "\n",
    "print(f\"Number of devices under strategy: {strategy.num_replicas_in_sync}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6095546,
     "sourceId": 9918587,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.804679,
   "end_time": "2024-11-15T20:49:46.518580",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-15T20:49:41.713901",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
