{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9919779,"sourceType":"datasetVersion","datasetId":6095546}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":4.804679,"end_time":"2024-11-15T20:49:46.51858","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-15T20:49:41.713901","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/alexandrenouarcto/nvidia-guide-ai?scriptVersionId=208935777\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# For example, here's several helpful packages to load\n\n\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-11-22T03:54:48.032257Z","iopub.execute_input":"2024-11-22T03:54:48.032614Z","iopub.status.idle":"2024-11-22T03:54:48.301003Z","shell.execute_reply.started":"2024-11-22T03:54:48.032583Z","shell.execute_reply":"2024-11-22T03:54:48.300169Z"},"papermill":{"duration":1.113012,"end_time":"2024-11-15T20:49:45.987628","exception":false,"start_time":"2024-11-15T20:49:44.874616","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/Updated_GPU_Dataset_with_Tensor_Core_Information copy_cleaned.csv\n/kaggle/input/Updated_GPU_Dataset_with_Tensor_Core_Information.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\nimport subprocess\n\nimport numpy as np\n\nimport pandas as pd\n\nimport gc\n\nimport ctypes\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n\n\n# Deep Learning Libraries\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\n\nfrom tensorflow.keras import backend as K, Model, Input, layers\n\nfrom tensorflow.keras.models import Sequential, Model, load_model\n\nfrom tensorflow.keras.layers import (Dense, Dropout, Conv1D, BatchNormalization, ReLU, MaxPooling1D, \n\n                                   GlobalAveragePooling1D, Add, Concatenate, Input, LayerNormalization, \n\n                                   MultiHeadAttention, Flatten, Reshape, LeakyReLU, Activation, Lambda, Layer)\n\nfrom tensorflow.keras.backend import clear_session\n\nfrom tensorflow.keras.losses import MeanSquaredError\n\nfrom tensorflow.keras.metrics import RootMeanSquaredError, Metric\n\nfrom tensorflow.keras.optimizers import Adam, RMSprop, Adamax\n\nfrom tensorflow.keras.callbacks import EarlyStopping, Callback, LearningRateScheduler\n\nfrom tensorflow.keras.mixed_precision import Policy, set_global_policy\n\nfrom tensorflow.keras.regularizers import l2, l1_l2\n\nfrom tensorflow.data import Dataset\n\nfrom keras import mixed_precision\n\n\n\n# Machine Learning Libraries\n\nfrom sklearn.cluster import DBSCAN\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.neighbors import NearestNeighbors\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, confusion_matrix\n\nfrom sklearn.model_selection import train_test_split, KFold\n\nfrom sklearn.preprocessing import StandardScaler as sklearn_StandardScaler\n\n\n\n# PySpark Libraries\n\nfrom pyspark.sql import SparkSession\n\nfrom pyspark.sql.functions import col, udf, monotonically_increasing_id, mean, stddev, when\n\nfrom pyspark.sql.types import ArrayType, FloatType, IntegerType, DoubleType, StringType\n\nfrom pyspark.ml.feature import (StringIndexer, OneHotEncoder, VectorAssembler as SparkVectorAssembler, \n\n                               MinMaxScaler as SparkMinMaxScaler, RobustScaler, \n\n                               StandardScaler as SparkStandardScaler, PCA as SparkPCA)\n\nfrom pyspark.ml import Pipeline\n\nfrom pyspark.ml.linalg import Vectors, DenseVector\n\nfrom pyspark.ml.regression import RandomForestRegressor\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n\n\n# Hyperparameter Tuning Libraries\n\nimport keras_tuner as kt\n\nfrom keras_tuner import HyperParameters, Hyperband\n\nimport optuna\n\nfrom optuna.integration import TFKerasPruningCallback\n\n\n\n# GPU Acceleration\n\nfrom numba import cuda\n\n\n\n# Visualization Libraries\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n\n\n# Utility Libraries\n\nimport logging\n\nfrom termcolor import colored\n\nfrom math import sqrt\n\nfrom itertools import count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:54:48.302344Z","iopub.execute_input":"2024-11-22T03:54:48.302682Z","iopub.status.idle":"2024-11-22T03:55:01.720562Z","shell.execute_reply.started":"2024-11-22T03:54:48.302657Z","shell.execute_reply":"2024-11-22T03:55:01.718395Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 75\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler \u001b[38;5;28;01mas\u001b[39;00m sklearn_StandardScaler\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# PySpark Libraries\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, udf, monotonically_increasing_id, mean, stddev, when\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayType, FloatType, IntegerType, DoubleType, StringType\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pyspark'","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"# 🎮 *NVIDIA GPU GUIDE FOR KAGGLE* 🚀\n\n\n\n## 🌟 A Magical Journey Through GPU Computing 🌟\n\n\n\n![NVIDIA RTX](https://img.shields.io/badge/NVIDIA-RTX-76B900?style=for-the-badge&logo=nvidia&logoColor=white)\n\n\n\n\n\n\n\n## 🎯 Choose Your Path:\n\n\n\n\n\n\n\n### 1. 🎮 The Gamer's Quest\n\n\n\n- Level up from gaming to ML/Data Science mastery\n\n\n\n- Harness your GPU's hidden potential\n\n\n\n\n\n\n\n### 2. 🤖 The AI Engineer's Saga \n\n\n\n- From development to production deployment\n\n\n\n- Craft powerful AI solutions\n\n\n\n\n\n\n\n### 3. ⚡ The HPC Wizard's Chronicle\n\n\n\n- Master the art of High Performance Computing\n\n\n\n- Unlock unprecedented computational power\n\n\n\n\n\n\n\n### 4. 🔥 The Enthusiast's Adventure\n\n\n\n- Push boundaries with extreme overclocking\n\n\n\n- Pioneer cutting-edge AI research\n\n\n\n\n\n\n\n![GPU Architecture](https://img.shields.io/badge/CUDA-Powered-76B900?style=for-the-badge&logo=nvidia&logoColor=white)","metadata":{"papermill":{"duration":0.001683,"end_time":"2024-11-15T20:49:45.991687","exception":false,"start_time":"2024-11-15T20:49:45.990004","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Initialize Spark session for the master node\n# 1 - Use all available local cores for the master node\n# 2 - Set the application name for the master node\n# 3 - Configure the driver and executor memory and GPU settings\nimport subprocess\nimport json\n\n\n# ======================================================================================================#\n# =========================================[INIT SPARK]=================================================#\n# ======================================================================================================#\n\n# Configure Spark with GPU settings for RTX 4090 (24GB) and A5000 (24GB)\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName(\"ERROR 418 - I'M A TEAPOT\") \\\n    .config(\"spark.driver.memory\", \"48g\") \\\n    .config(\"spark.executor.memory\", \"48g\") \\\n    .config(\"spark.driver.maxResultSize\", \"48g\") \\\n    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n    .config(\"spark.rapids.memory.pinnedPool.size\", \"24G\") \\\n    .config(\"spark.rapids.sql.concurrentGpuTasks\", \"2\") \\\n    .config(\"spark.rapids.memory.gpu.pooling.enabled\", \"true\") \\\n    .config(\"spark.rapids.memory.gpu.allocFraction\", \"0.95\") \\\n    .config(\"spark.rapids.sql.explain\", \"ALL\") \\\n    .config(\"spark.executor.resource.gpu.amount\", \"2\") \\\n    .config(\"spark.task.resource.gpu.amount\", \"0.25\") \\\n    .config(\"spark.rapids.sql.incompatibleOps.enabled\", \"true\") \\\n    .config(\"spark.rapids.memory.host.spillStorageSize\", \"42G\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n    .config(\"spark.rapids.memory.gpu.maxAllocFraction\", \"0.95\") \\\n    .config(\"spark.rapids.sql.batchSizeBytes\", \"512M\") \\\n    .config(\"spark.rapids.sql.reader.batchSizeRows\", \"10000\") \\\n    .config(\"spark.rapids.sql.variableRowGroupSize.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Set log level to INFO\nspark.sparkContext.setLogLevel(\"INFO\")\n# Configure logging\nlog4jLogger = spark._jvm.org.apache.log4j\nlogger = log4jLogger.LogManager.getLogger(__name__)\n\ndef custom_logger(level, message):\n    color = 'white'\n    if level == \"INFO\":\n        color = 'cyan'\n    elif level == \"SUCCESS\":\n        color = 'green'\n    elif level == \"ERROR\":\n        color = 'red'\n    elif level == \"ACTION\":\n        color = 'blue'\n    elif level == \"PROGRESS\" or level == \"WARNING\":\n        color = 'yellow'\n    elif level == \"FINAL\":\n        color = 'magenta'\n    logger.info(colored(f\"SPARK: {level} - {message}\", color))\n\n# ======================================================================================================#\n# =========================================[INIT CUDA]==================================================#\n# ======================================================================================================#\n\n# TF_GPU_ALLOCATOR: Controls the GPU memory allocator used by TensorFlow\n# Setting to 'cuda_malloc_async' enables asynchronous memory allocation for better performance\nos.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n\n# TF_FORCE_GPU_ALLOW_GROWTH: Controls whether TensorFlow allocates all GPU memory at once\n# Setting to 'true' makes memory allocation grow as needed instead of taking all at startup\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\n# CUDA_DEVICE_ORDER: Determines how CUDA devices are ordered\n# \"PCI_BUS_ID\" makes CUDA devices ordered by their PCI bus IDs for consistent device numbering\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n\n# CUDA_VISIBLE_DEVICES: Specifies which GPUs are visible to the application\n# \"0, 1\" makes only GPUs 0 and 1 visible to the application\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\ndef initialize_cuda():\n    \"\"\"\n    Initializes CUDA and returns handles for all available devices.\n    \n    Returns:\n        list: List of device handles\n    \"\"\"\n    # Load the CUDA library\n    cuda = ctypes.CDLL('libcuda.so')\n    \n    # Set up cuInit function for CUDA initialization\n    cuInit = cuda.cuInit\n    cuInit.restype = ctypes.c_int\n    cuInit.argtypes = [ctypes.c_uint]\n    \n    # Set up cuDeviceGetCount function to get number of CUDA devices\n    cuDeviceGetCount = cuda.cuDeviceGetCount\n    cuDeviceGetCount.restype = ctypes.c_int\n    cuDeviceGetCount.argtypes = [ctypes.POINTER(ctypes.c_int)]\n    \n    # Set up cuDeviceGet function to get handle to specific device\n    cuDeviceGet = cuda.cuDeviceGet\n    cuDeviceGet.restype = ctypes.c_int\n    cuDeviceGet.argtypes = [ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n\n    # Initialize CUDA driver API\n    res = cuInit(0)\n    if res != 0:\n        raise RuntimeError(\"Failed to initialize CUDA\")\n        \n    # Get count of available CUDA devices\n    device_count = ctypes.c_int()\n    res = cuDeviceGetCount(ctypes.byref(device_count))\n    if res != 0:\n        raise RuntimeError(\"Failed to get device count\")\n        \n    # Get handles for all available devices\n    devices = []\n    for i in range(device_count.value):\n        device = ctypes.c_int()\n        res = cuDeviceGet(ctypes.byref(device), i)\n        if res != 0:\n            raise RuntimeError(f\"Failed to get device {i}\")\n        devices.append(device.value)\n    return devices\n\ndef allocate_gpu_memory(size):\n    \"\"\"\n    Allocates GPU memory of a given size.\n    \n    Args:\n        size (int): Size of memory to allocate in bytes\n        \n    Returns:\n        int: Pointer to the allocated memory\n    \"\"\"\n    # Load CUDA library\n    cuda = ctypes.CDLL('libcuda.so')\n    \n    # Set up memory allocation function\n    cuda_malloc = cuda.cuMemAlloc\n    cuda_malloc.restype = ctypes.c_int\n    cuda_malloc.argtypes = [ctypes.POINTER(ctypes.c_ulonglong), ctypes.c_ulonglong]\n    \n    # Allocate memory on GPU\n    ptr = ctypes.c_ulonglong()\n    res = cuda_malloc(ctypes.byref(ptr), size)\n    if res != 0:\n        raise RuntimeError(\"Failed to allocate GPU memory\")\n    return ptr.value\n\ndef free_gpu_memory(ptr):\n    \"\"\"\n    Frees GPU memory allocated by allocate_gpu_memory.\n    \n    Args:\n        ptr (int): Pointer to the allocated memory\n    \"\"\"\n    # Load CUDA library\n    cuda = ctypes.CDLL('libcuda.so')\n    \n    # Set up memory deallocation function\n    cuda_free = cuda.cuMemFree\n    cuda_free.restype = ctypes.c_int\n    cuda_free.argtypes = [ctypes.c_ulonglong]\n    \n    # Free allocated GPU memory\n    res = cuda_free(ptr)\n    if res != 0:\n        raise RuntimeError(\"Failed to free GPU memory\")\n\n# Check GPU memory usage\ndef check_gpu_memory():\n    \"\"\"\n    Checks GPU memory usage by running nvidia-smi command.\n    \"\"\"\n    os.system('nvidia-smi')\n\n# Initialize CUDA\ncuda_devices = initialize_cuda()\n\n# Check available GPU memory\ncheck_gpu_memory()\n\n# Ensure TensorFlow uses the GPU by setting memory growth\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(f\"Error setting memory growth: {e}\")\n\n# Check and print the number of GPUs available\nnum_gpus = len(tf.config.list_physical_devices('GPU'))\nprint(f\"Num GPUs Available: {num_gpus}\")\n\n# Get current GPU memory usage using nvidia-smi\ndef get_gpu_memory_usage():\n    \"\"\"\n    Retrieves the current GPU memory usage in MB using nvidia-smi command.\n    \n    This function executes nvidia-smi with specific query parameters to get memory usage,\n    parses the output and returns it as an integer value.\n    \n    Returns:\n        int: The amount of GPU memory currently in use in MB.\n        None: If there's an error querying the GPU.\n    \"\"\"\n    try:\n        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        memory_used = int(result.stdout.decode('utf-8').strip())\n        return memory_used\n    except Exception as e:\n        print(f\"Failed to get GPU memory usage: {e}\")\n        return None\n\ndef freegpu():\n    \"\"\"\n    Forcefully frees GPU memory by killing processes using NVIDIA GPUs.\n    \n    This function:\n    1. Gets initial GPU memory usage as baseline\n    2. Finds all processes using NVIDIA devices using fuser command\n    3. Forcefully terminates these processes with SIGKILL (signal 9)\n    \n    Note: Requires sudo privileges to run fuser command\n    \n    Returns:\n        None. Prints memory freed or error messages.\n    \"\"\"\n    try:\n        # Get GPU memory usage before freeing for comparison\n        memory_before = get_gpu_memory_usage()\n\n        # Run fuser command to list processes using NVIDIA devices\n        result = subprocess.run(['sudo', 'fuser', '-v', '/dev/nvidia*'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        \n        # Parse output and kill each process using NVIDIA devices\n        for line in result.stdout.decode('utf-8').split('\\n'):\n            if '/dev/nvidia' in line:\n                pid = int(line.split()[-1])  # Extract process ID\n                os.kill(pid, 9)  # Send SIGKILL signal\n\n        # Get GPU memory usage after freeing\n        memory_after = get_gpu_memory_usage()\n\n        if memory_before is not None and memory_after is not None:\n            memory_freed = memory_before - memory_after\n            print(f\"GPU memory has been freed. {memory_freed} MB of GPU memory was released.\")\n        else:\n            print(\"GPU memory has been freed.\")\n    except Exception as e:\n        print(f\"Failed to free GPU memory: {e}\")\n\n# Set the environment variable for XLA flags\nos.environ['XLA_FLAGS'] = '--xla_gpu_strict_conv_algorithm_picker=false'\n\n# ======================================================================================================\n# ==============================[SETUP MIRROREDSTRATEGY WITHOUT NCCL]===================================\n# ======================================================================================================\n\n# ------------------------------------------------------------------------------------------------------\n# Define the strategy with HierarchicalCopyAllReduce to avoid NCCL\n# ------------------------------------------------------------------------------------------------------\nstrategy = tf.distribute.MirroredStrategy(\n    devices=[\"/gpu:0\", \"/gpu:1\"],\n    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()\n)\n\nprint(f\"Number of devices under strategy: {strategy.num_replicas_in_sync}\")","metadata":{"papermill":{"duration":0.00158,"end_time":"2024-11-15T20:49:45.99511","exception":false,"start_time":"2024-11-15T20:49:45.99353","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.721655Z","iopub.status.idle":"2024-11-22T03:55:01.722136Z","shell.execute_reply.started":"2024-11-22T03:55:01.721901Z","shell.execute_reply":"2024-11-22T03:55:01.721925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **LazyEval EDA**","metadata":{}},{"cell_type":"code","source":"# Load and analyze GPU dataset\ndef load_gpu_dataset():\n    \"\"\"\n    Load the GPU dataset from either Kaggle or local path.\n    Returns:\n        pd.DataFrame: Loaded GPU dataset\n    \"\"\"\n    kaggle_path = '/kaggle/input/Updated_GPU_Dataset_with_Tensor_Core_Information copy_cleaned.csv'\n    local_path = '/home/skander/datasets/gpu/Updated_GPU_Dataset_with_Tensor_Core_Information copy.csv'\n    \n    try:\n        # Try Kaggle path first\n        custom_logger(\"INFO\", \"Attempting to load from Kaggle path...\")\n        data = pd.read_csv(kaggle_path, index_col=False, header=0)\n        custom_logger(\"SUCCESS\", \"Dataset loaded successfully from Kaggle\")\n        return data\n    except FileNotFoundError:\n        try:\n            # Try local path if Kaggle fails\n            custom_logger(\"INFO\", \"Kaggle path failed. Attempting local path...\")\n            data = pd.read_csv(local_path, index_col=False, header=0)\n            custom_logger(\"SUCCESS\", \"Dataset loaded successfully from local path\")\n            return data\n        except FileNotFoundError:\n            custom_logger(\"ERROR\", \"Both Kaggle and local paths failed. Please check file paths.\")\n            return None\n\ndef univariate_analysis(df):\n    \"\"\"\n    Perform comprehensive univariate analysis on the DataFrame.\n    \n    Args:\n        df (pd.DataFrame): Input DataFrame for analysis\n        \n    Prints:\n        - Basic dataset information (shape, dtypes)\n        - Missing value analysis\n        - Duplicate entry analysis\n        - Numerical column statistics\n        - Detailed NaN value report\n    \"\"\"\n    custom_logger(\"ACTION\", \"Starting univariate analysis...\")\n    \n    # Dataset overview\n    custom_logger(\"INFO\", f\"Dataset Shape: {df.shape}\")\n    custom_logger(\"INFO\", \"\\nData Types:\")\n    print(df.dtypes)\n    \n    # Missing values analysis\n    missing_values = df.isnull().sum()\n    custom_logger(\"INFO\", \"\\nMissing Values Summary:\")\n    print(missing_values)\n    \n    # Duplicate analysis\n    duplicate_count = df.duplicated().sum()\n    custom_logger(\"INFO\", f\"\\nNumber of Duplicate Entries: {duplicate_count}\")\n    \n    # Numerical statistics\n    custom_logger(\"INFO\", \"\\nNumerical Column Statistics:\")\n    print(df.describe(include=[np.number]))\n    \n    # Detailed NaN analysis\n    custom_logger(\"INFO\", \"\\nDetailed NaN Value Analysis:\")\n    for col in df.columns:\n        if df[col].isnull().any():\n            nan_count = df[col].isnull().sum()\n            nan_percentage = df[col].isnull().mean() * 100\n            custom_logger(\"WARNING\", f\"Column '{col}': {nan_count} NaN values ({nan_percentage:.2f}%)\")\n    \n    custom_logger(\"FINAL\", \"Univariate analysis completed\")\n\n# Execute the analysis\noriginal_data = load_gpu_dataset()\nif original_data is not None:\n    custom_logger(\"INFO\", \"ORIGINAL DATA - BEFORE PROCESSING\")\n    univariate_analysis(original_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.723548Z","iopub.status.idle":"2024-11-22T03:55:01.723877Z","shell.execute_reply.started":"2024-11-22T03:55:01.723715Z","shell.execute_reply":"2024-11-22T03:55:01.723729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Lack of features - adding Tensor Cores, Generation, and End-of-Life**","metadata":{}},{"cell_type":"code","source":"from cudf import read_csv\n\n\ndf =  pd.read_csv('gpu_cleaned.csv')\n\n# Removing commas and converting columns to float\ncolumns_to_clean = [\"cuda_cores\", \"base_clock_mhz\", \"tdp\", \"fp16\", \"fp32\", \"fp64\", \"bandwith\"]\n\nfor column in columns_to_clean:\n    if df[column].dtype == 'object':  # Check if the column is a string\n        df[column] = df[column].str.replace(\",\", \"\").astype(float)\n    else:\n        df[column] = df[column].astype(float)\n\ndef dataset_info(df, name):\n    info = {\n        \"Head\": df.head(),\n        \"Tail\": df.tail(),\n        \"Shape\": df.shape,\n        \"Data Types\": df.dtypes,\n        \"Missing Values\": df.isnull().sum(),\n        \"Duplicates\": df.duplicated().sum()\n    }\n    print(f\"Dataset: {name}\")\n    for key, value in info.items():\n        print(f\"\\n{key}:\\n{value}\")\n    print(\"\\n\" + \"=\"*40 + \"\\n\")\n\n# Display information for each dataset\ndataset_info(df, \"tech specs\")\n\n# Copy the datasets to avoid any issues with the original data\ndf_cleaned = df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.724951Z","iopub.status.idle":"2024-11-22T03:55:01.725376Z","shell.execute_reply.started":"2024-11-22T03:55:01.725155Z","shell.execute_reply":"2024-11-22T03:55:01.725177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Safely process the 'cuda_cores' column\nif df_cleaned['cuda_cores'].dtype == 'object':  # Check if it's a string\n    df_cleaned['cuda_cores'] = df_cleaned['cuda_cores'].str.replace(',', '').astype(float)\nelse:\n    df_cleaned['cuda_cores'] = df_cleaned['cuda_cores'].astype(float)\n\n# List of numeric columns to analyze\ncolumns_to_analyze = ['price', 'vram', 'tensor_cores', 'cuda_cores', 'bandwith', 'tdp']\n\n# List of unique markets\nmarkets = df_cleaned['market'].unique()\n\n# Loop through each column and create visualizations per market\nfor col in columns_to_analyze:\n    plt.figure(figsize=(12, 6))\n    for market in markets:\n        subset = df_cleaned[df_cleaned['market'] == market]\n        plt.hist(subset[col], bins=20, alpha=0.5, label=market)\n    plt.title(f'{col.capitalize()} Distribution by Market')\n    plt.xlabel(col.capitalize())\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.727089Z","iopub.status.idle":"2024-11-22T03:55:01.727512Z","shell.execute_reply.started":"2024-11-22T03:55:01.727292Z","shell.execute_reply":"2024-11-22T03:55:01.727314Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🎮 GPU Market Analysis: Your Ultimate Guide! \n\n\n\n## For Gamers 🎯\n\nHey gamers! Here's what you need to know:\n\n\n\n### Price & Performance Sweet Spots 💰\n\n- Most gaming GPUs cluster under $2,000 - perfect for high-end builds!\n\n- The RTX 4090 is the current king but comes at a premium\n\n- Great value found in mid-range cards ($500-$1000)\n\n\n\n### What Really Matters for Gaming 🏆\n\n- VRAM: 8-16GB is the sweet spot for modern games\n\n- High memory bandwidth for those sweet FPS\n\n- CUDA cores still important for raw gaming performance\n\n- Look for cards with good DLSS support (Tensor cores help!)\n\n\n\n### Power & Cooling Tips ⚡\n\n- Gaming cards can be power hungry (especially high-end models)\n\n- Make sure your PSU can handle the peaks\n\n- Consider case airflow for these hot performers!\n\n\n\n---\n\n\n\n## For AI/ML Enthusiasts 🤖\n\nReady to train some models? Check this out:\n\n\n\n### Key Specs for ML Work 📊\n\n- VRAM is king! Look for 24GB+ for serious work\n\n- Tensor cores are your best friend\n\n- High memory bandwidth helps training speed\n\n\n\n### Best Options 🎯\n\n- Datacenter cards (A100, H100) are dream machines but $$$\n\n- Professional cards offer good balance\n\n- Some consumer cards (4090) can handle smaller models\n\n\n\n### Cost-Performance Tips 💡\n\n- Consider multiple consumer cards vs one datacenter card\n\n- Watch for cards with good FP16/BF16 support\n\n- Memory bandwidth per dollar is crucial\n\n\n\n---\n\n\n\n## For HPC Warriors 🚀\n\nRunning serious computations? Here's your guide:\n\n\n\n### Critical Features ⚡\n\n- ECC memory support for reliability\n\n- FP64 performance for scientific computing\n\n- Stable power draw for datacenter deployment\n\n\n\n### Market Analysis 📈\n\n- Datacenter GPUs dominate this space\n\n- Professional cards offer workstation-friendly options\n\n- Consumer cards generally lack required features\n\n\n\n### Deployment Considerations 🔧\n\n- Power efficiency is crucial at scale\n\n- Look for cards with good virtualization support\n\n- Consider cooling requirements in dense setups\n\n\n\n---\n\n\n\n## For Hardware Enthusiasts 🔧\n\nLove the technical details? Let's geek out!\n\n\n\n### Architecture Deep Dive 🎛️\n\n- Latest gen brings massive improvements in Tensor cores\n\n- Memory tech evolving: GDDR6X vs HBM2e\n\n- Power delivery systems getting more sophisticated\n\n\n\n### Interesting Trends 📊\n\n- Consumer cards catching up on compute features\n\n- Professional cards filling the middle ground\n\n- Legacy cards showing interesting price/performance ratios\n\n\n\n### Future Watch 🔮\n\n- Next-gen memory bandwidth improvements coming\n\n- AI acceleration becoming standard\n\n- Power efficiency vs performance battle continues\n\n\n\n---\n\n\n\n## 🌟 Quick Decision Guide\n\n\n\n### Choose Gaming if you need:\n\n- High FPS in latest games\n\n- Great price/performance ratio\n\n- DLSS/RT features\n\n\n\n### Choose AI/ML if you need:\n\n- Large VRAM pools\n\n- Top tensor performance\n\n- Training flexibility\n\n\n\n### Choose HPC if you need:\n\n- ECC memory\n\n- FP64 performance\n\n- Deployment reliability\n\n\n\n### Choose Professional if you need:\n\n- Balance of features\n\n- Certified drivers\n\n- Workstation reliability\n\n\n\n## 🎯 Next Steps\n\n1. **Compare Your Use Case**\n\n   - Match your needs to the right category\n\n   - Consider hybrid use cases\n\n   \n\n2. **Budget Planning**\n\n   - Set realistic expectations\n\n   - Consider TCO (Total Cost of Ownership)\n\n   \n\n3. **Stay Updated**\n\n   - Watch for new releases\n\n   - Monitor price trends\n\n   - Keep an eye on emerging technologies\n\n\n\nHappy GPU hunting! 🚀\n","metadata":{}},{"cell_type":"code","source":"# Convert numeric columns\n\nnumeric_cols = ['bandwith', 'fp16', 'fp32', 'fp64', 'tdp']\n\nfor col in numeric_cols:\n\n    # First convert to string, handle any non-numeric values, then convert to float\n\n    df_cleaned[col] = pd.to_numeric(\n\n        df_cleaned[col].astype(str).replace(',', '', regex=True),\n\n        errors='coerce'\n\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.728733Z","iopub.status.idle":"2024-11-22T03:55:01.729183Z","shell.execute_reply.started":"2024-11-22T03:55:01.728965Z","shell.execute_reply":"2024-11-22T03:55:01.728987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🚀 Tensor Core Evolution: Powering the AI Revolution\n\n# \n\n# Let's explore how NVIDIA's Tensor Core technology has evolved across generations and markets,\n\n# revolutionizing both AI/ML workloads and gaming performance through mixed-precision computing.\n\n#\n","metadata":{}},{"cell_type":"code","source":"# Convert numeric columns before grouping\n\nnumeric_cols = ['fp32', 'tensor_cores', 'cuda_cores', 'bus_width', 'bandwith', 'fp64', 'tdp', 'price', 'fp16']\n\ndf_numeric = df_cleaned.copy()\n\n\n\nfor col in numeric_cols:\n\n    # Remove any non-numeric characters and convert to numeric\n\n    df_numeric[col] = df_numeric[col].replace(r'[^\\d.]', '', regex=True)\n\n    df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')\n\n\n\n# Group by Tensor Core generation and market, only aggregating numeric columns\n\ntensor_core_impact = df_numeric.groupby(['tensor_core_gen', 'market'], as_index=False)[numeric_cols].mean()\n\n\n\n# Plot FP32 performance by Tensor Core generation\n\nplt.figure(figsize=(10, 6))\n\nsns.barplot(\n\n    data=tensor_core_impact,\n\n    x='tensor_core_gen', \n\n    y='fp32',\n\n    hue='market',\n\n    palette='muted'\n\n)\n\nplt.title('Impact of Tensor Core Generation on FP32 Performance')\n\nplt.xlabel('Tensor Core Generation')\n\nplt.ylabel('FP32 Performance (GFLOPs)')\n\nplt.legend(title='Market', bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.grid(axis='y')\n\nplt.show()\n\n\n\n\n\n# Plot FP16 performance by Tensor Core generation\n\nplt.figure(figsize=(10, 6))\n\nsns.barplot(\n\n    data=tensor_core_impact,\n\n    x='tensor_core_gen', \n\n    y='fp16',\n\n    hue='market',\n\n    palette='muted'\n\n)\n\nplt.title('Impact of Tensor Core Generation on FP16 Performance')\n\nplt.xlabel('Tensor Core Generation')\n\nplt.ylabel('FP16 Performance (GFLOPs)')\n\nplt.legend(title='Market', bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.grid(axis='y')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.730555Z","iopub.status.idle":"2024-11-22T03:55:01.730999Z","shell.execute_reply.started":"2024-11-22T03:55:01.730762Z","shell.execute_reply":"2024-11-22T03:55:01.730783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysis of Tensor Core Impact Across User Segments\n\n\n\n## Gaming Users\n\n- Tensor cores primarily benefit DLSS (Deep Learning Super Sampling) for higher FPS and resolution upscaling\n\n- 2nd Gen (Turing) and newer provide meaningful gaming benefits through DLSS 2.0/3.0\n\n- Most gamers don't need the extreme FP32/FP16 performance of datacenter cards\n\n- Sweet spot is RTX 30/40 series consumer cards with 3rd/4th gen tensor cores\n\n\n\n## AI/ML Users\n\n- Dramatic FP16 speedup from tensor cores essential for training/inference\n\n- 4th Gen (Ada Lovelace) shows massive ~60,000 GFLOPS FP32 in datacenter segment\n\n- Professional cards (A-series) balance memory and compute for development\n\n- Datacenter cards (H100 etc) optimal for large-scale training\n\n\n\n## HPC Users\n\n- Need maximum FP64 performance which tensor cores don't directly accelerate\n\n- Still benefit from mixed precision training using tensor cores\n\n- Datacenter cards with 4th/5th gen cores provide best performance\n\n- Memory bandwidth and size often more important than raw tensor performance\n\n\n\n## Hardware Enthusiasts\n\n- Can see clear generational improvements in tensor core capabilities\n\n- 4th gen shows biggest leap in both consumer and datacenter segments\n\n- Professional cards offer good balance of features for experimentation\n\n- Latest consumer cards (4090 etc) provide excellent price/performance\n\n\n\nKey Observations:\n\n- Each tensor core generation roughly doubles performance\n\n- Datacenter cards show highest absolute performance\n\n- Consumer cards focus on gaming/content creation\n\n- Professional cards balance various workloads\n\n- Legacy cards lack tensor cores entirely\n","metadata":{}},{"cell_type":"markdown","source":"# Exploring the Relationship Between CUDA Cores and FP32 Performance: A Deep Dive into NVIDIA's Raw Computing Power\n","metadata":{}},{"cell_type":"code","source":"# Scatter plot for CUDA Cores vs FP32 performance\n\nplt.figure(figsize=(20, 12), dpi=100)\n\n\n\nsns.scatterplot(\n\n    data=df_cleaned.sort_values('fp32', ascending=False),  # Sorted FP32\n\n    x='cuda_cores',\n\n    y='fp32',\n\n    hue='market',\n\n    style='market',\n\n    palette='deep',\n\n    alpha=0.7,\n\n    s=100\n\n)\n\n\n\n# Add GPU model labels for significant points\n\nfor idx, row in df_cleaned.iterrows():\n\n    if row['cuda_cores'] > 7000 or row['fp32'] > 30000:\n\n        plt.annotate(\n\n            row['model'],\n\n            (row['cuda_cores'], row['fp32']),\n\n            xytext=(5, 5),\n\n            textcoords='offset points',\n\n            fontsize=8,\n\n            alpha=0.7\n\n        )\n\n\n\n# Customize the plot\n\nplt.title('CUDA Cores vs FP32 Performance by Market Segment', fontsize=16)\n\nplt.xlabel('CUDA Cores', fontsize=14)\n\nplt.ylabel('FP32 Performance (GFLOPs)', fontsize=14)\n\nplt.grid(True, alpha=0.3)\n\nplt.legend(title='Market', title_fontsize=12, fontsize=10, bbox_to_anchor=(1.05, 1))\n\n\n\n# Adjust layout\n\nplt.tight_layout()\n\n\n\n# Show the plot\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.732508Z","iopub.status.idle":"2024-11-22T03:55:01.732777Z","shell.execute_reply.started":"2024-11-22T03:55:01.732649Z","shell.execute_reply":"2024-11-22T03:55:01.732662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CUDA Cores Analysis: Understanding NVIDIA's Strategy and Market Segmentation\n\n\n\nThe scatter plot above reveals fascinating insights about NVIDIA's approach to CUDA cores and performance:\n\n\n\n## Key Observations\n\n- Strong correlation between CUDA cores and FP32 performance, but not perfectly linear\n\n- Clear market segmentation visible through clustering\n\n- High-end cards (H100, A100) show dramatic increases in both metrics\n\n- Gaming cards follow a more moderate scaling curve\n\n\n\n## NVIDIA's Strategic Evolution\n\n1. **Datacenter Focus**: Massive core counts in datacenter GPUs (15,000+ CUDA cores)\n\n2. **Gaming/Consumer Balance**: Mid-range sweet spot around 4000-6000 CUDA cores\n\n3. **Professional Cards**: Balanced approach prioritizing reliability over raw core count\n\n\n\n## Important Note on CUDA Cores as Performance Metric\n\n⚠️ CUDA cores primarily indicate FP32 (32-bit floating point) performance only!\n\n- Not representative of:\n\n  - Tensor operations (ML/AI)\n\n  - Ray tracing capabilities\n\n  - Memory bandwidth\n\n  - Real-world application performance\n\n\n\n## Recommendations by User Segment\n\n\n\n### 🎮 Gamers\n\n- Focus on gaming-specific benchmarks rather than CUDA core count\n\n- Consider RTX features and memory bandwidth\n\n- Sweet spot: RTX 4070/4080 series for price/performance\n\n\n\n### 🤖 AI/ML Users\n\n- Prioritize Tensor Cores over CUDA cores\n\n- Consider memory capacity and bandwidth\n\n- Look for cards with latest Tensor Core generation\n\n- Recommended: A100/H100 for enterprise, RTX 4090 for individuals\n\n\n\n### 🖥️ HPC Users\n\n- CUDA cores matter more for traditional compute\n\n- Balance with memory subsystem capabilities\n\n- Consider professional cards (A-series) for certified workloads\n\n- Focus on specific application benchmarks\n\n\n\n### 🔧 Enthusiasts\n\n- Understand CUDA cores are just one piece of the puzzle\n\n- Consider total platform cost (power, cooling)\n\n- Look at performance per dollar rather than raw numbers\n\n- Research specific workload requirements before investing\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Add a column for FP32 performance per CUDA core\n\ndf_cleaned['fp32_per_core'] = df_cleaned['fp32'] / df_cleaned['cuda_cores']\n\n\n\n# Plot normalized performance\n\nplt.figure(figsize=(12, 6))\n\nsns.boxplot(data=df_cleaned, x='market', y='fp32_per_core', palette='muted')\n\nplt.title('Normalized FP32 Performance (FP32 per CUDA Core) by Market', fontsize=16)\n\nplt.xlabel('Market', fontsize=14)\n\nplt.ylabel('FP32 Performance per CUDA Core', fontsize=14)\n\nplt.grid(axis='y', alpha=0.3)\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.73416Z","iopub.status.idle":"2024-11-22T03:55:01.734423Z","shell.execute_reply.started":"2024-11-22T03:55:01.734295Z","shell.execute_reply":"2024-11-22T03:55:01.734308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3D scatter plot for FP32 vs CUDA Cores vs Tensor Cores\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n\nfig = plt.figure(figsize=(12, 8))\n\nax = fig.add_subplot(111, projection='3d')\n\nsc = ax.scatter(\n\n    df_cleaned['cuda_cores'], \n\n    df_cleaned['tensor_cores'], \n\n    df_cleaned['fp32'], \n\n    c=df_cleaned['market'].factorize()[0], \n\n    cmap='viridis', \n\n    s=50\n\n)\n\nax.set_title('FP32 vs CUDA Cores and Tensor Cores', fontsize=16)\n\nax.set_xlabel('CUDA Cores', fontsize=12)\n\nax.set_ylabel('Tensor Cores', fontsize=12)\n\nax.set_zlabel('FP32 Performance (GFLOPs)', fontsize=12)\n\nplt.colorbar(sc, label='Market')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.735334Z","iopub.status.idle":"2024-11-22T03:55:01.73561Z","shell.execute_reply.started":"2024-11-22T03:55:01.735465Z","shell.execute_reply":"2024-11-22T03:55:01.735478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a deep copy of the filtered dataset for each market\n\ndf_filtered = df_cleaned[df_cleaned['tensor_cores'] > 1].copy()\n\n\n\n# Apply log scaling to FP16 and Tensor Cores\n\ndf_filtered['log_fp16'] = np.log1p(df_filtered['fp16'])\n\ndf_filtered['log_tensor_cores'] = np.log1p(df_filtered['tensor_cores'])\n\n\n\n# Get unique markets\n\nmarkets = df_filtered['market'].unique()\n\n\n\n# Create subplots for each market\n\nfig, axes = plt.subplots(len(markets), 1, figsize=(14, 8 * len(markets)), sharex=True)\n\n\n\n# Iterate over each market and plot\n\nfor i, market in enumerate(markets):\n\n    ax = axes[i]\n\n    sns.scatterplot(\n\n        data=df_filtered[df_filtered['market'] == market],\n\n        x='log_tensor_cores',\n\n        y='log_fp16',\n\n        hue='tensor_core_gen',\n\n        style='tensor_core_gen',\n\n        alpha=0.8,\n\n        size='fp32',\n\n        sizes=(50, 300),\n\n        ax=ax\n\n    )\n\n    ax.set_title(f'Tensor Cores vs FP16 Performance (Log Scale) - {market}', fontsize=16)\n\n    ax.set_xlabel('Log(Tensor Cores)', fontsize=14)\n\n    ax.set_ylabel('Log(FP16 Performance)', fontsize=14)\n\n    ax.legend(title='Generation', fontsize=10, title_fontsize=12)\n\n    ax.grid(alpha=0.3)\n\n\n\n    # Add labels for GPUs\n\n    for _, row in df_filtered[df_filtered['market'] == market].iterrows():\n\n        ax.annotate(\n\n            row['model'],\n\n            (np.log1p(row['tensor_cores']), np.log1p(row['fp16'])),\n\n            xytext=(5, 5),\n\n            textcoords='offset points',\n\n            fontsize=8,\n\n            alpha=0.7\n\n        )\n\n\n\n# Adjust layout\n\nplt.tight_layout()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.736981Z","iopub.status.idle":"2024-11-22T03:55:01.737247Z","shell.execute_reply.started":"2024-11-22T03:55:01.737118Z","shell.execute_reply":"2024-11-22T03:55:01.73713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CUDA vs Tensor Cores: The Battle for AI Supremacy - Analyzing FP16 & FP32 Performance Across GPU Generations\n\n# \n\n# This analysis explores the relationship between traditional CUDA cores and specialized Tensor cores,\n\n# examining their impact on FP16 and FP32 performance. We'll investigate which core type proves more\n\n# instrumental for different workloads and how this has evolved across GPU generations.\n","metadata":{}},{"cell_type":"markdown","source":"## **FP32 Breakdown**","metadata":{}},{"cell_type":"code","source":"# Create a 3D scatter plot to show FP32 vs. Memory Bandwidth vs. Tensor Cores\n\nfig = plt.figure(figsize=(20, 12))\n\nax = fig.add_subplot(111, projection='3d')\n\n\n\n# Scatter plot\n\nscatter = ax.scatter(\n\n    df_cleaned['bandwith'],\n\n    df_cleaned['fp32'],\n\n    df_cleaned['tensor_cores'],\n\n    c=df_cleaned['tensor_core_gen'].factorize()[0],  # Color by market\n\n    cmap='viridis',\n\n    s=50,  # Marker size\n\n    alpha=0.8\n\n)\n\n\n\n# Add labels for key GPUs (those with high FP32 or bandwidth)\n\nfor idx, row in df_cleaned.iterrows():\n\n    if row['fp32'] > 50000 or row['bandwith'] > 900:  # Filter for high values\n\n        ax.text(\n\n            row['bandwith'],\n\n            row['fp32'],\n\n            row['tensor_cores'],\n\n            row['model'],\n\n            fontsize=8,\n\n            alpha=0.7\n\n        )\n\n\n\n# Set axis labels\n\nax.set_xlabel('Memory Bandwidth (GB/s)', fontsize=12)\n\nax.set_ylabel('FP32 Performance (GFLOPs)', fontsize=12)\n\nax.set_zlabel('Tensor Cores', fontsize=12)\n\nax.set_title('FP32 vs. Memory Bandwidth vs. Tensor Cores by Market', fontsize=14)\n\n\n\n# Add color legend\n\nhandles, labels = scatter.legend_elements()\n\nmarkets = df_cleaned['tensor_core_gen'].unique()\n\n# Fix the legend labels by ensuring they are within bounds\n\nlegend_labels = []\n\nfor label in labels:\n\n    # Extract just the numeric part from the mathdefault string\n\n    num = label.split('{')[1].split('}')[0]\n\n    index = int(num)\n\n    if index < len(markets):  # Check if index is within bounds\n\n        legend_labels.append(markets[index])\n\n    else:\n\n        legend_labels.append('Unknown')  # Handle out-of-bounds index\n\nax.legend(handles, legend_labels, title='Market', loc='upper left', fontsize=10)\n\n\n\n# Display the plot\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.738601Z","iopub.status.idle":"2024-11-22T03:55:01.739067Z","shell.execute_reply.started":"2024-11-22T03:55:01.738813Z","shell.execute_reply":"2024-11-22T03:55:01.738834Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🚀 NVIDIA GPU Architecture Evolution: A Deep Dive into FP32 Performance & Tensor Cores\n\n\n\nLooking at our stunning 3D visualization above, let's explore the fascinating journey of NVIDIA's GPU architecture evolution! \n\n\n\n## 🔄 The Tensor Core Revolution\n\nOur plot reveals an incredible story of innovation in tensor core technology:\n\n- 🌟 **Early Days**: The yellow points show the pre-AI era - no tensor cores yet!\n\n- 📈 **Smart Scaling**: Rather than just adding more cores, NVIDIA took a sophisticated approach\n\n- 🔄 **Generational Leaps**: Watch the progression from Volta ➡️ Ampere ➡️ Ada Lovelace, each bringing smarter, not just more numerous, cores\n\n\n\n## 💪 FP32 Performance: Power Meets Precision\n\nThe visualization shows some impressive patterns:\n\n- 🏆 **Performance Kings**: Look at those H100 and A100 cards dominating the top of the chart!\n\n- 🎯 **Smart Architecture**: Notice how performance jumps come from better design, not just brute force\n\n- 📊 **Market Segmentation**: Clear performance tiers show NVIDIA's strategic product positioning\n\n\n\n## 🌊 Memory Bandwidth: The Silent Powerhouse\n\nThe data tells an interesting story about memory optimization:\n\n- 🔗 **Performance Link**: See how higher bandwidth and better FP32 performance go hand in hand\n\n- 💼 **Data Center Focus**: Tesla series GPUs showcase the perfect balance of bandwidth and compute\n\n- 🎮 **Consumer Sweet Spot**: Gaming cards find that perfect middle ground\n\n\n\n## 🔍 Key Technical Breakthroughs\n\n1. 🎯 **Efficiency First**: NVIDIA prioritizes smarter cores over more cores\n\n2. 🌟 **Volta's Legacy**: The introduction of tensor cores changed everything\n\n3. 📈 **Generational Improvements**:\n\n   - Ampere: Brought sparsity support and precision flexibility\n\n   - Ada Lovelace: Delivered optimized matrix ops and better power efficiency\n\n4. 🏢 **Data Center Innovation**: Proving that architectural finesse beats brute force\n\n\n\n> 💡 **Key Takeaway**: NVIDIA's journey shows us that in GPU architecture, working smarter beats working harder. Each generation brings more efficient tensor cores rather than just more of them.\n","metadata":{}},{"cell_type":"code","source":"# Create a 3D scatter plot to show FP16 vs. Memory Bandwidth vs. Tensor Cores\n\nfig = plt.figure(figsize=(20, 12))\n\nax = fig.add_subplot(111, projection='3d')\n\n\n\n# Scatter plot\n\nscatter = ax.scatter(\n\n    df_cleaned['bandwith'],\n\n    df_cleaned['fp16'],\n\n    df_cleaned['tensor_cores'],\n\n    c=df_cleaned['tensor_core_gen'].factorize()[0],  # Color by market\n\n    cmap='viridis',\n\n    s=50,  # Marker size\n\n    alpha=0.8\n\n)\n\n\n\n# Add labels for key GPUs (those with high FP16 or bandwidth)\n\nfor idx, row in df_cleaned.iterrows():\n\n    if row['fp16'] > 50000 or row['bandwith'] > 900:  # Filter for high values\n\n        ax.text(\n\n            row['bandwith'],\n\n            row['fp16'],\n\n            row['tensor_cores'],\n\n            row['model'],\n\n            fontsize=8,\n\n            alpha=0.7\n\n        )\n\n\n\n# Set axis labels\n\nax.set_xlabel('Memory Bandwidth (GB/s)', fontsize=12)\n\nax.set_ylabel('FP16 Performance (GFLOPs)', fontsize=12)\n\nax.set_zlabel('Tensor Cores', fontsize=12)\n\nax.set_title('FP16 vs. Memory Bandwidth vs. Tensor Cores by Market', fontsize=14)\n\n\n\n# Add color legend\n\nhandles, labels = scatter.legend_elements()\n\nmarkets = df_cleaned['tensor_core_gen'].unique()\n\n# Fix the legend labels by ensuring they are within bounds\n\nlegend_labels = []\n\nfor label in labels:\n\n    # Extract just the numeric part from the mathdefault string\n\n    num = label.split('{')[1].split('}')[0]\n\n    index = int(num)\n\n    if index < len(markets):  # Check if index is within bounds\n\n        legend_labels.append(markets[index])\n\n    else:\n\n        legend_labels.append('Unknown')  # Handle out-of-bounds index\n\nax.legend(handles, legend_labels, title='Market', loc='upper left', fontsize=10)\n\n\n\n# Display the plot\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.740067Z","iopub.status.idle":"2024-11-22T03:55:01.740474Z","shell.execute_reply.started":"2024-11-22T03:55:01.740262Z","shell.execute_reply":"2024-11-22T03:55:01.740283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Filter for recent GPUs (2016-2024)\n\ndf_filtered = df_cleaned[\n\n    (pd.to_datetime(df_cleaned['release_date'], format='mixed', errors='coerce') < '2025-01-01') & \n\n    (pd.to_datetime(df_cleaned['release_date'], format='mixed', errors='coerce') > '2016-01-01')\n\n].copy()\n\n\n\n# Convert numeric columns more carefully\n\nnumeric_columns = ['tensor_cores', 'fp32', 'cuda_cores', 'bus_width', 'bandwith', 'fp64', 'tdp', 'price']\n\nfor col in numeric_columns:\n\n    # Remove any non-numeric characters and convert\n\n    df_filtered[col] = df_filtered[col].replace(r'[^\\d.]', '', regex=True)\n\n    df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce')\n\n    # Fill with median value for that column\n\n    df_filtered[col] = df_filtered[col].fillna(df_filtered[col].median())\n\n\n\n# Create figure and subplots first\n\nfig = plt.figure(figsize=(80, 80))\n\ngs = plt.GridSpec(10, 4, figure=fig)\n\nax1 = fig.add_subplot(gs[0, 0])\n\nax2 = fig.add_subplot(gs[0, 1])\n\n\n\n# Create scatter plots without labels in the scatterplot call\n\nscatter1 = sns.scatterplot(data=df_filtered,\n\n                x='bandwith',\n\n                y='tensor_cores',\n\n                hue='market',\n\n                size='tdp',\n\n                alpha=0.7,\n\n                ax=ax1)\n\n\n\nscatter2 = sns.scatterplot(data=df_filtered,\n\n                x='fp64',\n\n                y='tdp',\n\n                hue='market', \n\n                size='tdp',\n\n                alpha=0.7,\n\n                ax=ax2)\n\n\n\n# Set titles for the plots\n\nax1.set_title('GPU Performance')\n\nax2.set_title('GPU Efficiency')\n\n\n\n# Add legends with proper positioning\n\nax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\nax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n\n\n# Plot 3: Memory Configuration Analysis\n\nax3 = fig.add_subplot(gs[1, 0])\n\nvalid_market_labels = df_filtered['market'].dropna().unique()\n\ndf_plot = df_filtered[df_filtered['market'].isin(valid_market_labels)]\n\nsns.boxplot(data=df_plot,\n\n            x='market',\n\n            y='bus_width',\n\n            ax=ax3)\n\nax3.set_title('Memory Interface Analysis by Segment')\n\nax3.set_xlabel('Market Segment')\n\nax3.set_ylabel('Memory Bus Width (bits)')\n\nax3.tick_params(axis='x', rotation=45)\n\n\n\n# Plot 4: Price Efficiency Matrix\n\nprice_metrics = ['tensor_cores_per_dollar', 'fp64_per_dollar', 'bandwidth_per_dollar']\n\ncorrelation_matrix = df_filtered[price_metrics].corr()\n\nax4 = fig.add_subplot(gs[1, 1])\n\nsns.heatmap(correlation_matrix,\n\n            annot=True,\n\n            cmap='coolwarm',\n\n            ax=ax4)\n\nax4.set_title('Price Efficiency Correlation Matrix')\n\n\n\n# Plot 5: Price vs Performance Timeline\n\nax5 = fig.add_subplot(gs[2, :])\n\nmetrics = ['tensor_cores_per_dollar', 'fp64_per_dollar', 'bandwidth_per_dollar']\n\nfor metric in metrics:\n\n    sns.lineplot(data=df_filtered,\n\n                x='release_date',\n\n                y=metric,\n\n                label=metric,\n\n                ax=ax5)\n\nax5.set_title('Evolution of Price-Performance Ratio Over Time')\n\nax5.set_xlabel('Release Date')\n\nax5.set_ylabel('Performance per Dollar')\n\nax5.tick_params(axis='x', rotation=45)\n\nax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n\n\n# Print detailed analysis for different workloads\n\nprint(\"\\n=== GPU Workload Analysis ===\")\n\n\n\n# AI/ML Optimized GPUs\n\nprint(\"\\nBest GPUs for AI/ML Training:\")\n\nai_metrics = ['model', 'tensor_cores', 'bandwith', 'tdp', 'price', 'tensor_cores_per_dollar', 'bandwidth_per_dollar']\n\ntop_ai = df_filtered.sort_values('tensor_cores', ascending=False).head(3)\n\nfor _, row in top_ai[ai_metrics].iterrows():\n\n    print(f\"\\n{row['model']}:\")\n\n    print(f\"- Tensor Cores: {int(row['tensor_cores'])}\")\n\n    print(f\"- Memory Bandwidth: {int(row['bandwith'])} GB/s\")\n\n    print(f\"- Power Draw: {int(row['tdp'])}W\")\n\n    print(f\"- Price: ${int(row['price'])}\")\n\n    print(f\"- Tensor Cores per Dollar: {row['tensor_cores_per_dollar']:.2f}\")\n\n    print(f\"- Bandwidth per Dollar: {row['bandwidth_per_dollar']:.2f} GB/s/$\")\n\n\n\n# HPC Optimized GPUs\n\nprint(\"\\nBest GPUs for HPC Workloads:\")\n\nhpc_metrics = ['model', 'fp64', 'bandwith', 'tdp', 'price', 'fp64_per_dollar', 'bandwidth_per_dollar']\n\ntop_hpc = df_filtered.sort_values('fp64', ascending=False).head(3)\n\nfor _, row in top_hpc[hpc_metrics].iterrows():\n\n    print(f\"\\n{row['model']}:\")\n\n    print(f\"- FP64 Performance: {row['fp64']} TFLOPS\")\n\n    print(f\"- Memory Bandwidth: {int(row['bandwith'])} GB/s\")\n\n    print(f\"- Power Draw: {int(row['tdp'])}W\")\n\n    print(f\"- Price: ${int(row['price'])}\")\n\n    print(f\"- FP64 TFLOPS per Dollar: {row['fp64_per_dollar']:.2f}\")\n\n    print(f\"- Bandwidth per Dollar: {row['bandwidth_per_dollar']:.2f} GB/s/$\")\n\n\n\n# Gaming/Consumer GPUs\n\nprint(\"\\nBest Consumer GPUs:\")\n\nconsumer_metrics = ['model', 'cuda_cores', 'bandwith', 'tdp', 'price', 'bandwidth_per_dollar']\n\ntop_consumer = df_filtered[df_filtered['market'] == 'consumer'].sort_values('bandwith', ascending=False).head(3)\n\nfor _, row in top_consumer[consumer_metrics].iterrows():\n\n    print(f\"\\n{row['model']}:\")\n\n    print(f\"- CUDA Cores: {int(row['cuda_cores'])}\")\n\n    print(f\"- Memory Bandwidth: {int(row['bandwith'])} GB/s\")\n\n    print(f\"- Power Draw: {int(row['tdp'])}W\")\n\n    print(f\"- Price: ${int(row['price'])}\")\n\n    print(f\"- Bandwidth per Dollar: {row['bandwidth_per_dollar']:.2f} GB/s/$\")\n\n\n\n# Best Value Analysis\n\nprint(\"\\n=== Best Value GPUs ===\")\n\nprint(\"\\nBest Value for AI/ML (Tensor Cores per Dollar):\")\n\ntop_ai_value = df_filtered.sort_values('tensor_cores_per_dollar', ascending=False).head(3)\n\nfor _, row in top_ai_value[['model', 'tensor_cores_per_dollar', 'price', 'market']].iterrows():\n\n    print(f\"{row['model']} ({row['market']}): {row['tensor_cores_per_dollar']:.2f} cores/$ (Price: ${int(row['price'])})\")\n\n\n\nprint(\"\\nBest Value for HPC (FP64 per Dollar):\")\n\ntop_hpc_value = df_filtered.sort_values('fp64_per_dollar', ascending=False).head(3)\n\nfor _, row in top_hpc_value[['model', 'fp64_per_dollar', 'price', 'market']].iterrows():\n\n    print(f\"{row['model']} ({row['market']}): {row['fp64_per_dollar']:.2f} TFLOPS/$ (Price: ${int(row['price'])})\")\n\n\n\nprint(\"\\nBest Value for Memory Bandwidth (GB/s per Dollar):\")\n\ntop_bandwidth_value = df_filtered.sort_values('bandwidth_per_dollar', ascending=False).head(3)\n\nfor _, row in top_bandwidth_value[['model', 'bandwidth_per_dollar', 'price', 'market']].iterrows():\n\n    print(f\"{row['model']} ({row['market']}): {row['bandwidth_per_dollar']:.2f} GB/s/$ (Price: ${int(row['price'])})\")\n\n\n\n# Adjust layout with sufficient margins\n\nplt.subplots_adjust(top=0.95, bottom=0.05, left=0.1, right=0.9)\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T03:55:01.741952Z","iopub.status.idle":"2024-11-22T03:55:01.742365Z","shell.execute_reply.started":"2024-11-22T03:55:01.742148Z","shell.execute_reply":"2024-11-22T03:55:01.74217Z"}},"outputs":[],"execution_count":null}]}